{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science project for the 13/04/2019 event\n",
    "## purpose of the project\n",
    "\n",
    "Le but de ce projet est de scraper les donnees pour faire une presentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules for our work\n",
    "#importing numpy\n",
    "import numpy as np\n",
    "# importing matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# importing pandas\n",
    "import pandas as pd\n",
    "# importing request\n",
    "import requests\n",
    "import os, os.path\n",
    "# importing beautifull soup\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "#importing the time module\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following we will create a text file where we can store our links so that later we can scrap again and do ### comparaison on our data.\n",
    "\n",
    "# the cell below should be run whenever you want to scrap new links because the intention is to save the link scraped at a particular time in orther to be able to rescrapped the same to analyse over time because if we don't do that each time we will be having different results etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: 1\n",
      "Scraping page: 2\n",
      "Scraping page: 3\n",
      "Scraping page: 4\n",
      "Scraping page: 5\n",
      "Scraping page: 6\n",
      "Scraping page: 7\n",
      "Scraping page: 8\n",
      "Scraping page: 9\n",
      "Scraping page: 10\n",
      "Scraping page: 11\n",
      "Scraping page: 12\n",
      "Scraping page: 13\n",
      "Scraping page: 14\n",
      "Scraping page: 15\n",
      "Scraping page: 16\n",
      "Scraping page: 17\n",
      "Scraping page: 18\n",
      "Scraping page: 19\n",
      "Scraping page: 20\n",
      "Scraping page: 21\n",
      "Scraping page: 22\n",
      "Scraping page: 23\n",
      "Scraping page: 24\n",
      "Scraping page: 25\n"
     ]
    }
   ],
   "source": [
    "# url that will be scrapped\n",
    "url = 'https://www.jumia.cm/smartphones/'\n",
    "#pages_to_crawl = 20\n",
    "#number of pages that will be scrapped\n",
    "# here we are scrapping 25 pages which represent the number of pages under smartphones\n",
    "pages_to_crawl = 25\n",
    "r= requests.get(url)\n",
    "#soup = BeautifulSoup(r.text, 'html.parser')\n",
    "''' the for look bellow help get the link of each item and we can open each to investigate more in porder to get insight\n",
    "from the data store in it'''\n",
    "#links = soup.find_all(class_=\"link\")\n",
    "# getting all the links and putting them in an array\n",
    "\n",
    "\n",
    "# scarping the pages\n",
    "# here we define a for loop that will loop throught all the pages and save the link into an array\n",
    "#delete the old file and create a new one\n",
    "if os.path.exists('links.txt'):\n",
    "    os.remove(\"links.txt\")\n",
    "f=open(\"links.txt\",\"a+\")\n",
    "for p in range(1, pages_to_crawl+1):\n",
    "    print('Scraping page:', p)\n",
    "    r = requests.get(url, params={'page' : p})\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for found  in soup.findAll('a', attrs={'class':'link', 'href': re.compile(\"^https://\")}):\n",
    "        link1=found.get('href')\n",
    "        f.write(link1+'\\n')\n",
    "f.close()\n",
    "    # we pause our code for a second so that we should not be flaged as spam\n",
    "   # time.sleep(1)\n",
    "#     for found  in soup.findAll('a', attrs={'class':'link', 'href': re.compile(\"^https://\")}):\n",
    "#         link1=found.get('href')\n",
    "#         with open ('links', 'w+') as link:\n",
    "#             link.write(link1+'\\n')\n",
    "#print (links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to fix in other to have  a good scrapper\n",
    "\n",
    "After examining the data scrapped it turned out that there are some data scrapped that the scrapper did not figure it out well. Here are the list of behaviour to fix.\n",
    "1. number-of-items-sold: the scrapper seemed to performing well here but towards the end we don't have the value meanwhile it was supposed to be 0 if there was nothing so we need to figure out why its not working. I think since the scrapper is taking too long(more than 30 minutes) we can just scrapped the last page in other to tery to understand why\n",
    "2. same issue with rom and ram but this can be solved by understand and solving the dual appearance of the of the value scrapped\n",
    "3. fix the camera scrapper too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difining a dictionarry of data that need to be scrapped\n",
    "data ={'name':[],\n",
    "      'seller':[],\n",
    "      'price':[],\n",
    "      'number-of-items-sold': [],\n",
    "      'ram': [],\n",
    "      'rom': [],\n",
    "      'screen-size': [],\n",
    "       'color': [],\n",
    "       'front-camera':[],\n",
    "       'rear-camera':[],\n",
    "       'number_of_sims':[],\n",
    "       'date': [],\n",
    "       'os':[]\n",
    "      }\n",
    "\n",
    "\n",
    "\n",
    "#next after saving the link to each item, we open each of the and perform the data extraction on them\n",
    "\n",
    "with open('links.txt') as f:\n",
    "    for line in f:\n",
    "        # open the link using the get method\n",
    "        item_link = requests.get(line.strip())\n",
    "        item_link_soup =BeautifulSoup(item_link.text, 'html.parser')\n",
    "\n",
    "        ''' make sure that the seller name exist since this is the brand of the phone , make sure that the brand name is not empty and also that the brand name is different from rock since we have realize that rock, universal is not a phone brand\n",
    "        and also the type generic is the type of every other item not phones'''\n",
    "        if item_link_soup.find('div', {'class':'sub-title'}).find('a') is not None and item_link_soup.find('div', {'class':'sub-title'}).find('a').get_text(strip=True).lower() !='generic' and (item_link_soup.find('div', {'class':'sub-title'}).find('a').get_text(strip=True).lower() !='universal') and (item_link_soup.find('div', {'class':'sub-title'}).find('a').get_text(strip=True).lower() !='rock'):\n",
    "            #getting the seller name after inpection of the link we realize that we could get it from the a tag\n",
    "            vendeur = item_link_soup.find('div', {'class':'sub-title'}).find('a').get_text(strip=True)\n",
    "            data['seller'].append(vendeur)\n",
    "            name = item_link_soup.find('h1', {'class': 'title' }).get_text(strip=True)\n",
    "            #we realize that in the name of the product, the number of sims cards is indicated but not hence we will check if it specy it if not we put it at 1\n",
    "            if('dual sim' in name.lower() or 'double puce' in name.lower()):\n",
    "                data['number_of_sims'].append(2)\n",
    "            else:\n",
    "                data['number_of_sims'].append(1)\n",
    "            data['name'].append(name)\n",
    "            #getting the price of items\n",
    "            prise = item_link_soup.find('span', {'dir':'ltr'})\n",
    "            prize = prise.get_text(strip=True).replace(',', '')\n",
    "            data['price'].append(float(prize))\n",
    "\n",
    "            # getting the numbers of items sold\n",
    "            sold = item_link_soup.find('div', {'class':'-ptxxs'})\n",
    "            if sold  is not None :\n",
    "                if sold.find('span', {'class': 'text color-default bold -prxs -inline-block -bold'})is not None:\n",
    "                    vendu = sold.find('span', {'class': 'text color-default bold -prxs -inline-block -bold'}).get_text(strip=True)\n",
    "\n",
    "\n",
    "                # on the website the number of items sold are presents in form of X+ or <x so we add or substract one to reflect it\n",
    "                    if vendu.endswith('+'):\n",
    "                        vendu1 = vendu.replace('+', '')\n",
    "                        vendu2 = vendu1.replace(',', '')\n",
    "                        vendu_final = float(vendu2) + 1\n",
    "                        data['number-of-items-sold'].append(vendu_final)\n",
    "                    # here due to the fact that jumia.cm doesn't give the exact number sold, we add 1 if its said to be more than or sousract if to be less than.\n",
    "                    elif vendu.startswith('<'):\n",
    "                        vendu1 = vendu.replace('<', '')\n",
    "                        vendu2 = vendu1.replace(',', '')\n",
    "                        vendu_final = float(vendu2) - 1\n",
    "                        data['number-of-items-sold'].append(vendu_final)\n",
    "                    else :\n",
    "                        data['number-of-items-sold'].append(0)\n",
    "            # this else can be removed was having an error in my notebook and thought it was the problem but actually no\n",
    "            else:\n",
    "                ok = 'not know'\n",
    "\n",
    "            #getting the color of the phone and various other \n",
    "            features = []\n",
    "\n",
    "            for caracteristiques in item_link_soup.find_all('div', {'class':'osh-row '}):\n",
    "                value = caracteristiques.find_all('div', {'class': 'osh-col'})\n",
    "                features.append(value)\n",
    "\n",
    "\n",
    "            # we loop over the features and get the necessary info\n",
    "            #print(features)\n",
    "            for i in range(len(features)):\n",
    "                    #print(value1)\n",
    "                    # we unpack the value of the array\n",
    "                    try:\n",
    "                        x, y = features[i]\n",
    "                        if 'couleur principale' in x.get_text(strip=True).lower():\n",
    "                            #couleur = value[1].get_text(strip=True).lower()\n",
    "                            #print(couleur)\n",
    "                            data['color'].append(y.get_text(strip=True).lower())\n",
    "                            #print(value)\n",
    "                        elif 'taille de' in x.get_text(strip=True).lower():\n",
    "                             data['screen-size'].append(y.get_text(strip=True).lower())\n",
    "                        elif 'ram syst' in x.get_text(strip=True).lower():\n",
    "                            ram2 = float(y.get_text(strip=True).lower())\n",
    "                            # if the value of the ram is 512 it means that its 512mb and we convert it accordingly\n",
    "                            if ram2 ==512:\n",
    "                                data['ram'].append(0.512)\n",
    "                            else:\n",
    "                                data['ram'].append(float(y.get_text(strip=True).lower()))\n",
    "                        elif 'capacit' in x.get_text(strip=True).lower():\n",
    "                             data['rom'].append(float(y.get_text(strip=True).lower()))\n",
    "                        elif 'os' in x.get_text(strip=True).lower():\n",
    "                             data['os'].append(y.get_text(strip=True).lower())\n",
    "                        else:\n",
    "                            a = 'ok'\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            '''getting the camera info the site does not specify it in a simple way that we can just scrap so we are getting the\n",
    "            the camera info by using the regex''' \n",
    "            if(len(item_link_soup.find_all('ul')[4].find_all('li'))>2):\n",
    "                cam1 = str(item_link_soup.find_all('ul')[4].find_all('li')[2].get_text(strip= True)).lower()\n",
    "                if('cam√©ra' in cam1 or 'photo' in cam1):\n",
    "                    arr1 = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', cam1)]\n",
    "                    arr2 = [x for x in arr1 if x != 0.0 and x<26]\n",
    "                    if len(arr2) == 2 and arr2[1]<25:\n",
    "                        data['rear-camera'].append(arr2[0])\n",
    "                        data['front-camera'].append(arr2[1])\n",
    "                    elif len(arr2) == 2 and arr2[1]>25:\n",
    "                        data['rear-camera'].append(arr2[0])\n",
    "                        data['front-camera'].append(0)\n",
    "                    elif len(arr2)==1 :\n",
    "                        data['rear-camera'].append(arr2[0])\n",
    "                        data['front-camera'].append(0)\n",
    "                    elif len(arr2)==3 and arr2[1]<26 and arr2[2]<26:\n",
    "                        data['rear-camera'].append(arr2[0]+arr2[1])\n",
    "                        data['front-camera'].append(2)\n",
    "                    elif len(arr2)>3:\n",
    "                        data['rear-camera'].append(arr2[0])\n",
    "                        data['front-camera'].append(0)\n",
    "                    elif len(arr2)==4:\n",
    "                        data['rear-camera'].append(arr2[0]+arr2[1])\n",
    "                        data['front-camera'].append(arr2[2]+arr2(3))\n",
    "                    else:\n",
    "                        arr3 = 0\n",
    "            #data['vendeur'].append('unknow')\n",
    "            # we are using sleep in other for our scrapper to waith for one second before going again\n",
    "            #time.sleep(1)\n",
    "            scrap_time =datetime.datetime.now()\n",
    "            data['date'].append(scrap_time.strftime(\"%c\"))\n",
    "            # at the end the scrapper wait for 0.1 second before startint with the next link\n",
    "            time.sleep(0.100)\n",
    "\n",
    "''' We are having thinks twice so that s why we are dividing by two.'''\n",
    "        \n",
    "# casting colors in other to get the real color\n",
    "'''couleur_to =list( data['color'])\n",
    "couleur_ok = []\n",
    "ram_ok = []\n",
    "rom_ok = []\n",
    "taille_ok = []\n",
    "#print(couleur_to)\n",
    "ram_to = list(data['ram'])\n",
    "print(ram_to)\n",
    "rom_to = list(data['rom'])\n",
    "taille_to = list(data['screen-size'])\n",
    "print(int(len(couleur_to)/2))\n",
    "for i in range(int(len(taille_to)/2)):\n",
    "    taille_ok.append(taille_to[i*2])\n",
    "    data['screen-size'] = taille_ok\n",
    "for i in range(int(len(ram_to)/2)):\n",
    "    ram_ok.append(ram_to[i*2])\n",
    "    data['ram'] = ram_ok\n",
    "for i in range(int(len(rom_to)/2)):\n",
    "    rom_ok.append(rom_to[i*2])\n",
    "    data['rom'] = rom_ok\n",
    "for i in range(int(len(couleur_to)/2)):\n",
    "    \n",
    "    #del couleur_to[i*2]\n",
    "    couleur_ok.append(couleur_to[i*2])\n",
    "    #ram_ok.append(ram_to[i*2])\n",
    "    #rom_ok.append(rom_to[i*2])\n",
    "    #taille_ok.append(taille_to[i*2])\n",
    "    data['color'] = couleur_ok '''\n",
    "\n",
    "#print(data)\n",
    "\n",
    "#converting the dictionnary to a dataframe\n",
    "frame = pd.concat([pd.Series(v, name=k) for k, v in data.items()], axis=1)\n",
    "fileName = fileName = 'jumia_cm_smartphones_scraped_on_' + datetime.datetime.now().strftime(\"%c\").replace(' ', '_')+('.csv')\n",
    "#print(frame)\n",
    "# here we export the data to csv file\n",
    "export_csv = frame.to_csv (fileName, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysing the csv file save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  name   seller     price  \\\n",
      "905  Galaxy Note 8 - 64 Go HDD - 6 Go RAM - 6,3'' -...  Samsung  275000.0   \n",
      "906  Galaxy S7 Edge 64Go HDD - 4Go RAM - Or - Poche...  Samsung  149500.0   \n",
      "907  Galaxy S7 - 32 Go HDD - 4 Go RAM - 5,1'' - Or ...  Samsung  129500.0   \n",
      "908                Chargeur Pour Samsung Galaxy - Noir  Samsung    5000.0   \n",
      "909  Galaxy S9 Plus - 64Go HDD - 6Go RAM - 6,2\" - N...  Samsung  390000.0   \n",
      "\n",
      "     number-of-items-sold  ram  rom  screen-size color  front-camera  \\\n",
      "905                   NaN  NaN  NaN          NaN  noir           NaN   \n",
      "906                   NaN  NaN  NaN          NaN    or           NaN   \n",
      "907                   NaN  NaN  NaN          NaN    or           NaN   \n",
      "908                   NaN  NaN  NaN          NaN  noir           NaN   \n",
      "909                   NaN  NaN  NaN          NaN  noir           NaN   \n",
      "\n",
      "     rear-camera  number_of_sims                      date   os  \n",
      "905          NaN               1  Sun May 26 21:46:58 2019  NaN  \n",
      "906          NaN               1  Sun May 26 21:47:00 2019  NaN  \n",
      "907          NaN               1  Sun May 26 21:47:04 2019  NaN  \n",
      "908          NaN               1  Sun May 26 21:47:06 2019  NaN  \n",
      "909          NaN               1  Sun May 26 21:47:08 2019  NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 910 entries, 0 to 909\n",
      "Data columns (total 13 columns):\n",
      "name                    910 non-null object\n",
      "seller                  910 non-null object\n",
      "price                   910 non-null float64\n",
      "number-of-items-sold    863 non-null float64\n",
      "ram                     796 non-null float64\n",
      "rom                     824 non-null float64\n",
      "screen-size             768 non-null float64\n",
      "color                   910 non-null object\n",
      "front-camera            797 non-null float64\n",
      "rear-camera             797 non-null float64\n",
      "number_of_sims          910 non-null int64\n",
      "date                    910 non-null object\n",
      "os                      872 non-null object\n",
      "dtypes: float64(7), int64(1), object(5)\n",
      "memory usage: 92.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# reading the saved file\n",
    "df = pd.read_csv('jumia_cm_smartphones_scraped_on_Sun_May_26_21:47:08_2019.csv')\n",
    "print(df.tail())\n",
    "print(df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
